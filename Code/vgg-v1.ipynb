{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport os\nimport cv2\n\nimport torch\nfrom torch import nn\nfrom torchvision import transforms,models\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler \n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim import lr_scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----------parameter------------\nSEED = 42\nEPOCHS = 10\nLR = 1e-4\nMIN_LR = 1e-8\nMODE = 'min'\nFACTOR = 0.5\nPATIENCE =0\nBATCH_SIZE = 128\nTEST_SIZE = 0.2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/eda-plant-pathology-2021/train.csv')\ndf.label.value_counts()\ndd = []\nfor index,img in df[df.label==11].iterrows():\n    if img.image[:2] in['30','31','32','33','34','35','36','37','38','39']:\n        dd.append(index)\nfor index,img in df[df.label==2].iterrows():\n    if img.image[:2] in['38','39']:\n        dd.append(index)\nfor idx in dd:\n    df =df.drop(idx)\nfor _ in range(5):\n    df = df.sample(frac=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data split","metadata":{}},{"cell_type":"code","source":"# def data_split(phase = 'train',TEST_SIZE = 0.2):\n#     x_train, x_val, y_train, y_val = train_test_split(df.image,df.label_ex,\n#                                                       random_state = SEED,\n#                                                       shuffle=True,\n#                                                       test_size=TEST_SIZE,\n#                                                       stratify =df.label_ex)\n#     tar_csv = pd.DataFrame()\n#     if phase in ['train']:\n#         tar_csv['image'] = x_train\n#         tar_csv['label'] = y_train\n#     elif phase in ['val']:\n#         tar_csv['image'] = x_val\n#         tar_csv['label'] = y_val\n#     elif phase in ['test']:\n#         DIR = '../input/plant-pathology-2021-fgvc8/sample_submission.csv'\n#         tar_csv = pd.read_csv(DIR)\n    \n#     return tar_csv\ntrain_csv = df\nval_csv = pd.read_csv('../input/eda-plant-pathology-2021/val.csv')\nprint(f'The test size is {TEST_SIZE}\\nThe length of train set is {len(train_csv)}')\nprint(f'The length of validation set is {len(val_csv)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check weights","metadata":{}},{"cell_type":"code","source":"# def train_weight(train_csv):\n#     numarr = train_csv.label.value_counts().sort_index().values\n#     weights = 1.0/torch.tensor(numarr,dtype = torch.float)\n#     train_target = train_csv.label.tolist()\n#     sample_weights = weights[train_target]\n#     return sample_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# transform","metadata":{}},{"cell_type":"code","source":"class pl_transform():\n    def __init__(self):\n        self.plant_transform = {\n            'train':transforms.Compose([\n                transforms.CenterCrop(224),\n#                 transforms.RandomHorizontalFlip(),\n#                 transforms.RandomVerticalFlip(0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ]),  # ([0.47955528, 0.6252535, 0.4016591], [0.1559643, 0.13600954, 0.16537014])\n          \n            'val':A.Compose([\n  #              A.CLAHE(clip_limit=4.0, tile_grid_size=(8,8), p=1),\n                A.Resize(224,224),\n                A.CenterCrop(224,224),\n                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ToTensorV2()\n            ])\n        }\n        \n    def __call__(self, img, phase = 'train'):\n        if phase =='train':\n            return self.plant_transform[phase](img)\n        else:\n            img = np.array(img)\n            return self.plant_transform[phase](image=img)['image']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class mydataset(Dataset):\n    def __init__(self , csv_file , img_dir , transforms=None, phase = 'train' ):\n        self.targetfile = csv_file\n        self.root = img_dir\n        self.transforms = transforms\n        self.phase = phase\n\n    \n    def __len__(self):\n        return len(self.targetfile)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root,self.targetfile.iloc[idx,0])\n        image = Image.open(img_path)\n        label = self.targetfile.iloc[idx,1]\n        \n        if self.transforms:\n            image = self.transforms(image,self.phase)\n        return image,label\n\nROOT_TRAIN = '../input/eda-plant-pathology-2021/aug_re_img'\nROOT_VAL = '../input/resized-plant2021/img_sz_256'\ntrain_dataset = mydataset(train_csv,ROOT_TRAIN,pl_transform())\nval_dataset = mydataset(val_csv,ROOT_VAL,pl_transform(), phase = 'val')\n\n\nindex = 0\n\nprint(\"【train dataset】\")\nprint(f\"img num : {train_dataset.__len__()}\")\nprint(f\"img : {train_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {train_dataset.__getitem__(index)[1]}\")\nprint(\"\\n【validation dataset】\")\nprint(f\"img num : {val_dataset.__len__()}\")\nprint(f\"img : {val_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {val_dataset.__getitem__(index)[1]}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweights = train_weight(train_csv)\n# wsampler = WeightedRandomSampler(weights=tweights, num_samples=len(tweights), replacement=True)\ntrain_loader = DataLoader(train_dataset,\n#                        sampler = wsampler,\n                          batch_size = BATCH_SIZE,\n                          shuffle = True\n                         )\nval_loader = DataLoader(val_dataset,\n                          batch_size = BATCH_SIZE,\n                        shuffle = False\n                         )\n\nloader = {\"train\": train_loader, \"val\": val_loader}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # https://blog.csdn.net/PanYHHH/article/details/107896526\n# def getStat(train_data):\n#     '''\n#     Compute mean and variance for training data\n#     :return: (mean, std)\n#     '''\n#     print('Compute mean and variance for training data.')\n#     print(len(train_data))\n#     train_loader = torch.utils.data.DataLoader(\n#         train_data, batch_size=1, shuffle=False, num_workers=0,\n#         pin_memory=True)\n#     mean = torch.zeros(3)\n#     std = torch.zeros(3)\n#     for X, _ in tqdm(train_loader):\n#         for d in range(3):\n#             mean[d] += X[:, d, :, :].mean()\n#             std[d] += X[:, d, :, :].std()\n#     mean.div_(len(train_data))\n#     std.div_(len(train_data))\n\n#     return list(mean.numpy()), list(std.numpy())\n\n# DIR = '../input/plant-pathology-2021-fgvc8/train_images'\n# mydata =  mydataset(df , DIR, transforms =pl_transform(),phase ='calculate_meanstd')\n# lst  = getStat(mydata)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load model","metadata":{}},{"cell_type":"code","source":"use_pretrained = True\nMymodel = models.vgg16(pretrained=use_pretrained)\n\nMymodel.classifier[6] =  nn.Linear(in_features=4096, out_features=12)\nMymodel.classifier[2] = nn.Dropout(p=0.3, inplace=False)\nMymodel.classifier[5] = nn.Dropout(p=0.3, inplace=False)\nMymodel.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the parameters to be learned by finetuning in the variable params_to_update.\nparams_to_update_1 = []\nparams_to_update_2 = []\nparams_to_update_3 = []\n\n# Specify the parameter name of the layer to be trained.\nupdate_param_names_1 = [\"features.19.weight\", \"features.19.bias\",\"features.21.weight\", \"features.21.bias\",\"features.24.weight\", \"features.24.bias\", \"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\"]\nupdate_param_names_2 = [\"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\"]\nupdate_param_names_3 = [\"classifier.6.weight\", \"classifier.6.bias\"]\n\nfor name, param in Mymodel.named_parameters():\n    if name in update_param_names_1:\n        param.requires_grad = True\n        params_to_update_1.append(param)\n        print(f\"Store in params_to_update_1 : {name}\")\n    elif name in update_param_names_2:\n        param.requires_grad = True\n        params_to_update_2.append(param)\n        print(f\"Store in params_to_update_2 : {name}\")\n    elif name in update_param_names_3:\n        param.requires_grad = True\n        params_to_update_3.append(param)\n        print(f\"Store in params_to_update_3 : {name}\")\n    else:\n        param.requires_grad = False\n        print(f\"Parameters not to be learned :  {name}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------op&loss-----------\nMymodel.to(device)\n\nloss_fn=nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam([\n    {\"params\": params_to_update_1},\n    {\"params\": params_to_update_2},\n    {\"params\": params_to_update_3}\n],lr =LR)\n\nsgdr_partial = lr_scheduler.ReduceLROnPlateau(optimizer, mode=MODE, factor=FACTOR, \n                                              patience=PATIENCE,min_lr=MIN_LR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(net, loader, criterion, optimizer,sgdr_partial, num_epochs):\n    \"\"\"\n    Function for training the model.\n    \n    Parameters\n    ----------\n    net: object\n    dataloaders_dict: dictionary\n    criterion: object\n    optimizer: object\n    num_epochs: int\n    \"\"\"\n    print(f\"Devices to be used : {device}\")\n    torch.backends.cudnn.benchmark = True\n    # loop for epoch\n    train_acc = []\n    val_acc = []\n            \n    train_loss = []\n    val_loss = []\n    \n    lr = []\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1} / {num_epochs}\")\n        print(\"-------------------------------\")\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                net.train()\n            else:\n                net.eval()\n            epoch_loss = 0.0\n            epoch_corrects = 0\n#             if (epoch == 0) and (phase == \"train\"):\n#                 continue\n            f1lst_pred=[]\n            f1lst_true = []\n            for inputs, labels in tqdm(loader[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = net(inputs)\n                    loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs, 1)\n                    if phase =='val':\n                        f1lst_pred+= preds.data.tolist()\n                        f1lst_true+= labels.data.tolist()\n                    #print(num)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                        #sgdr_partial.step()\n                    epoch_loss += loss.item() * inputs.size(0)\n                    epoch_corrects += torch.sum(preds == labels.data)\n                \n                 \n            epoch_loss = epoch_loss / len(loader[phase].dataset)\n            epoch_acc = epoch_corrects.double() / len(loader[phase].dataset)\n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n            if phase == 'train':\n                train_acc.append(epoch_acc)\n                train_loss.append(epoch_loss)\n#                 train_acc += epoch_acc.tolist()\n#                 train_loss += epoch_loss.tolist()\n            \n            if phase =='val':\n                print(f\"The f1 score is {f1_score(f1lst_pred,f1lst_true,average = 'weighted')}\")\n                print(f\"Learning Rate is {optimizer.param_groups[0]['lr']}\")\n                sgdr_partial.step(epoch_loss)\n                val_acc.append(epoch_acc)\n                val_loss.append(epoch_loss)\n                lr.append(optimizer.param_groups[0]['lr'])\n    fig = plt.figure(figsize=(7, 6))\n    plt.grid(True)\n    plt.plot(train_acc, color='r',marker='o', label='train/acc')\n    plt.plot(val_acc, color='b',marker='x',label='val/acc')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    fig = plt.figure(figsize=(7, 6))\n    plt.grid(True)\n    plt.plot(train_loss, color='r',marker='o', label='train/loss')\n    plt.plot(val_loss, color='b',marker='x',label='val/loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.legend(loc='upper right')\n    plt.show()\n\n    fig = plt.figure(figsize=(7, 6))\n    plt.grid(True)\n    plt.plot(lr, color='g',marker='o',label='learning rate')\n    plt.ylabel('LR')\n    plt.xlabel('Epochs')\n    plt.legend(loc='upper right')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(Mymodel, loader, loss_fn, optimizer,sgdr_partial, EPOCHS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = \"./vgg16_fine_tuning_v1.h\"\ntorch.save(Mymodel.state_dict(), save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}