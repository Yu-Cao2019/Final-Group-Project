{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport os\nimport cv2\n\nimport torch\nfrom torch import nn\nfrom torchvision import transforms,models\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler \nfrom torch.optim import lr_scheduler\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/new-eda/train.csv')\ndf.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in range(5):\n    df = df.sample(frac=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----------parameter------------\nSEED = 42\nEPOCHS = 6\nLR = 1e-5\nMIN_LR = 1e-7\nMODE = 'min'\nFACTOR = 0.2\nPATIENCE = 0\nBATCH_SIZE = 128\nTEST_SIZE = 0.2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def data_split(phase = 'train',size =0.2):\n#     x_train, x_val, y_train, y_val = train_test_split(df.image,df.label_ex,\n#                                                       random_state = SEED,\n#                                                       shuffle=True,\n#                                                       test_size=size,\n#                                                       stratify =df.label_ex)\n#     tar_csv = pd.DataFrame()\n#     if phase in ['train']:\n#         tar_csv['image'] = x_train\n#         tar_csv['label'] = y_train\n#     elif phase in ['val']:\n#         tar_csv['image'] = x_val\n#         tar_csv['label'] = y_val\n#     elif phase in ['test']:\n#         DIR = '../input/plant-pathology-2021-fgvc8/sample_submission.csv'\n#         tar_csv = pd.read_csv(DIR)\n    \n#     return tar_csv\n\ntrain_csv = df\nval_csv = pd.read_csv('../input/new-eda/val.csv')\nprint(f'The test size is {TEST_SIZE}\\nThe length of train set is {len(train_csv)}')\nprint(f'The length of validation set is {len(val_csv)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_weight(train_csv):\n#     numarr = train_csv.label.value_counts().sort_index().values\n#     weights = 1.0/torch.tensor(np.log(numarr),dtype = torch.float)\n#     train_target = train_csv.label.tolist()\n#     sample_weights = weights[train_target]\n#     return sample_weights\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class pl_transform():\n    def __init__(self):\n        self.plant_transform = {\n            'train':transforms.Compose([\n                transforms.RandomResizedCrop(\n                    224, scale=(0.5, 1.0)),\n#                 transforms.RandomHorizontalFlip(),\n#                 transforms.RandomVerticalFlip(0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ]),  # ([0.47955528, 0.6252535, 0.4016591], [0.1559643, 0.13600954, 0.16537014])\n          \n            'val':transforms.Compose([\n                transforms.Resize(224),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ]),\n        }\n        \n    def __call__(self, img, phase = 'train'):\n\n        return self.plant_transform[phase](img)\n#         else:\n#             img =np.array(img)\n#             return self.plant_transform[phase](image = img)['image']\n\nclass mydataset(Dataset):\n    def __init__(self , csv_file , img_dir , transforms=None, phase = 'train' ):\n        self.targetfile = csv_file\n        self.root = img_dir\n        self.transforms = transforms\n        self.phase = phase\n\n    \n    def __len__(self):\n        return len(self.targetfile)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root,self.targetfile.iloc[idx,0])\n        image = Image.open(img_path)\n        label = self.targetfile.iloc[idx,1]\n        \n        if self.transforms:\n            image = self.transforms(image,self.phase)\n        return image,label\n\nROOT_TRAIN = '../input/new-eda/aug_re_img'\nROOT_VAL = '../input/resized-plant2021/img_sz_256'\ntrain_dataset = mydataset(train_csv,ROOT_TRAIN,pl_transform())\nval_dataset = mydataset(val_csv,ROOT_VAL,pl_transform(), phase = 'val')\n\n\nindex = 0\n\nprint(\"【train dataset】\")\nprint(f\"img num : {train_dataset.__len__()}\")\nprint(f\"img : {train_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {train_dataset.__getitem__(index)[1]}\")\nprint(\"\\n【validation dataset】\")\nprint(f\"img num : {val_dataset.__len__()}\")\nprint(f\"img : {val_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {val_dataset.__getitem__(index)[1]}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweights = train_weight(train_csv)\n# wsampler = WeightedRandomSampler(weights=tweights, num_samples=len(tweights), replacement=True)\ntrain_loader = DataLoader(train_dataset,\n#                        sampler = wsampler,\n                          batch_size = BATCH_SIZE,\n                          shuffle = True,\n                         )\nval_loader = DataLoader(val_dataset,\n                        batch_size = BATCH_SIZE,\n                        shuffle = False,\n                         )\n\nloader = {\"train\": train_loader, \"val\": val_loader}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_define():\n    use_pretrained = True\n    Mymodel = models.resnet18(pretrained=use_pretrained)\n    Mymodel.fc = nn.Sequential(\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 12)\n        )\n    \n    params_to_update_1 = []\n    update_param_names_1 = ['fc.1.weight','fc.1.bias']\n    for name, param in Mymodel.named_parameters():\n        if name in update_param_names_1:\n            param.requires_grad = True\n            params_to_update_1.append(param)\n            print(f\"Store in params_to_update_1 : {name}\")\n        elif name[5:8] in ['4.1']:\n            param.requires_grad = True\n            params_to_update_1.append(param)\n            print(f\"Store in params_to_update_1 : {name}\")\n        else:\n            param.requires_grad = False\n            print(f\"Parameters not to be learned :  {name}\")\n        \n\n    Mymodel.to(device)\n    return  params_to_update_1,  Mymodel\nparams_to_update_1,Mymodel = model_define()\nMymodel.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn=nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam([\n    {\"params\": params_to_update_1}] , lr =LR)\n\nsgdr_partial = lr_scheduler.StepLR(optimizer, step_size =4, gamma=0.1 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(net, loader, criterion, optimizer,sgdr_partial, num_epochs):\n    \"\"\"\n    Function for training the model.\n    \n    Parameters\n    ----------\n    net: object\n    dataloaders_dict: dictionary\n    criterion: object\n    optimizer: object\n    num_epochs: int\n    \"\"\"\n    print(f\"Devices to be used : {device}\")\n    torch.backends.cudnn.benchmark = True\n    # loop for epoch\n    train_acc = []\n    val_acc = []\n            \n    train_loss = []\n    val_loss = []\n    \n    lr = []\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1} / {num_epochs}\")\n        print(\"-------------------------------\")\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                net.train()\n            else:\n                net.eval()\n            epoch_loss = 0.0\n            epoch_corrects = 0\n#             if (epoch == 0) and (phase == \"train\"):\n#                 continue\n            f1lst_pred=[]\n            f1lst_true = []\n            for inputs, labels in tqdm(loader[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = net(inputs)\n                    loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs, 1)\n                    if phase =='val':\n                        f1lst_pred+= preds.data.tolist()\n                        f1lst_true+= labels.data.tolist()\n                    #print(num)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                        #sgdr_partial.step()\n                    epoch_loss += loss.item() * inputs.size(0)\n                    epoch_corrects += torch.sum(preds == labels.data)\n                \n                 \n            epoch_loss = epoch_loss / len(loader[phase].dataset)\n            epoch_acc = epoch_corrects.double() / len(loader[phase].dataset)\n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n            if phase == 'train':\n                train_acc.append(epoch_acc)\n                train_loss.append(epoch_loss)\n#                 train_acc += epoch_acc.tolist()\n#                 train_loss += epoch_loss.tolist()\n            \n            if phase =='val':\n                print(f\"The f1 score is {f1_score(f1lst_pred,f1lst_true,average = 'weighted')}\")\n                print(f\"Learning Rate is {optimizer.param_groups[0]['lr']}\")\n                sgdr_partial.step()\n                val_acc.append(epoch_acc)\n                val_loss.append(epoch_loss)\n                lr.append(optimizer.param_groups[0]['lr'])\n    fig = plt.figure(figsize=(7, 6))\n    plt.grid(True)\n    plt.plot(train_acc, color='r',marker='o', label='train/acc')\n    plt.plot(val_acc, color='b',marker='x',label='val/acc')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    fig = plt.figure(figsize=(7, 6))\n    plt.grid(True)\n    plt.plot(train_loss, color='r',marker='o', label='train/loss')\n    plt.plot(val_loss, color='b',marker='x',label='val/loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.legend(loc='upper right')\n    plt.show()\n\n    fig = plt.figure(figsize=(7, 6))\n    plt.grid(True)\n    plt.plot(lr, color='g',marker='o',label='learning rate')\n    plt.ylabel('LR')\n    plt.xlabel('Epochs')\n    plt.legend(loc='upper right')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(Mymodel, loader, loss_fn, optimizer,sgdr_partial, EPOCHS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = \"./resnet18_fine_tuning_v1.h\"\ntorch.save(Mymodel.state_dict(), save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root = '../input/resized-plant2021/img_sz_384'\n# for img in os.listdir(root):\n    \n#     img_path = os.path.join(root,img)\n#     image = cv2.imread(img_path)\n#     #image=np.array(image)\n#     print(image.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}